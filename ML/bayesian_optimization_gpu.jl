"""
CUDAå¤šGPUå¹¶è¡Œè´å¶æ–¯ä¼˜åŒ–æ¨¡å— (GPU-Accelerated Bayesian Optimization)

åŸºäºgpu_parallel_optimized.jlå®ç°çš„é«˜æ€§èƒ½è´å¶æ–¯ä¼˜åŒ–ï¼Œè§£å†³å•çº¿ç¨‹è¯„ä¼°ç“¶é¢ˆ

ä¸»è¦æ”¹è¿›ï¼š
1. æ‰¹é‡GPUå¹¶è¡Œç›®æ ‡å‡½æ•°è¯„ä¼°
2. æ™ºèƒ½æ‰¹æ¬¡ç®¡ç†å’Œå†…å­˜ä¼˜åŒ–
3. å¤šGPUå¼‚æ­¥å¤„ç†æ”¯æŒ
4. ä¿æŒåŸæœ‰è´å¶æ–¯ä¼˜åŒ–ç®—æ³•ä¸å˜
5. å…¼å®¹ç°æœ‰é…ç½®å’Œæ¥å£

è®¾è®¡å“²å­¦ï¼š
- "å¥½å“å‘³"ï¼šæ‰¹é‡è¯„ä¼°æ¶ˆé™¤å•ç‚¹ä¸²è¡Œçš„ç‰¹æ®Šæƒ…å†µ
- "Never break userspace"ï¼šå®Œå…¨å…¼å®¹ç°æœ‰BayesianOptimizeræ¥å£
- å®ç”¨ä¸»ä¹‰ï¼šGPUåŠ é€ŸçœŸæ­£çš„è®¡ç®—ç“¶é¢ˆï¼ˆODEæ±‚è§£ï¼‰
- ç®€æ´æ‰§å¿µï¼šæœ€å°åŒ–CPU-GPUæ•°æ®ä¼ è¾“æ¬¡æ•°
"""

using CUDA
using Statistics
using LinearAlgebra
using Random
using Plots
using JLD2
using Printf
using TOML
using Distributions

# å¼•å…¥ä¾èµ–æ¨¡å—
include("bayesian_optimization.jl")
include("gpu_parallel_optimized.jl")
include("surrogate_model.jl")

# åœ¨æ— æ˜¾ç¤ºç¯å¢ƒä¸‹ï¼Œå¼ºåˆ¶GRèµ°headlessæ¨¡å¼å¹¶é¿å…å­—ä½“é—®é¢˜
try
    ENV["GKSwstype"] = "100"
    default(fontfamily="sans")
catch
end

"""
    _row_key(x::AbstractVector{Float64})

ä¸ºæµ®ç‚¹å‘é‡ç”Ÿæˆç¨³å®šçš„å»é‡é”®ï¼Œé¿å…æµ®ç‚¹æ¯”è¾ƒè¯¯å·®ã€‚
"""
function _row_key(x::AbstractVector{Float64})
    io = IOBuffer()
    for i in axes(x, 1)
        @printf(io, "%.12g,", x[i])
    end
    return String(take!(io))
end

"""
    robust_optimize_acquisition(base::BayesianOptimizer; n_starts=8)

å¤šå¯åŠ¨é‡‡é›†å‡½æ•°ä¼˜åŒ–ï¼Œå¤±è´¥åˆ™å›é€€ä¸º nothingã€‚
"""
function robust_optimize_acquisition(base::BayesianOptimizer; n_starts::Int=8)
    best_x = nothing
    best_val = -Inf
    for _ in 1:n_starts
        x = nothing
        try
            x = optimize_acquisition_function(base)
        catch
            x = nothing
        end
        if x === nothing
            continue
        end
        # ç²—ç•¥ç”¨é‡‡é›†å€¼è¯„ä¼°æ’åºæ ‡å‡†
        val = try
            evaluate_acquisition_function(base, x)
        catch
            -Inf
        end
        if isfinite(val) && val > best_val
            best_val = val
            best_x = x
        end
    end
    return best_x
end

"""
deduplicate_training_data!(base_optimizer::BayesianOptimizer)

ç§»é™¤å®Œå…¨é‡å¤çš„è®­ç»ƒæ ·æœ¬ï¼Œé¿å…Krigingå› é‡å¤æ ·æœ¬æŠ¥é”™ã€‚
"""
function deduplicate_training_data!(base_optimizer::BayesianOptimizer)
    X = base_optimizer.X_evaluated
    y = base_optimizer.y_evaluated
    n = size(X, 1)
    if n <= 1
        return
    end
    seen = Set{String}()
    keep_indices = Int[]
    keep_indices_sizehint = sizehint!(keep_indices, n)
    for i in 1:n
        k = _row_key(view(X, i, :))
        if !(k in seen)
            push!(seen, k)
            push!(keep_indices, i)
        end
    end
    if length(keep_indices) < n
        base_optimizer.X_evaluated = X[keep_indices, :]
        base_optimizer.y_evaluated = y[keep_indices]
    end
end

 

"""
    _lhs_sample(n::Int, ranges) -> Matrix{Float64}

ç®€æ˜“Latin Hypercube Samplingï¼ŒèŒƒå›´æŒ‰å‚æ•°åŒºé—´æ˜ å°„ã€‚
"""
function _lhs_sample(n::Int, ranges)
    d = length(ranges)
    X = zeros(Float64, n, d)
    for j in 1:d
        lo, hi = minimum(ranges[j]), maximum(ranges[j])
        # å‡åŒ€åˆ†æ¡¶å¹¶éšæœºæ‰“ä¹±
        edges = range(0.0, 1.0; length = n + 1)
        # åœ¨æ¯ä¸ªæ¡¶å†…é€‰æ‹©ä¸€ä¸ªéšæœºç‚¹
        vals = [rand(edges[i]:(edges[i+1])) for i in 1:n]
        shuffle!(vals)
        for i in 1:n
            X[i, j] = lo + vals[i] * (hi - lo)
        end
    end
    return X
end

"""
    GPUBayesianConfig

GPUåŠ é€Ÿè´å¶æ–¯ä¼˜åŒ–é…ç½®
"""
Base.@kwdef struct GPUBayesianConfig
    # ç»§æ‰¿åŸºç¡€è´å¶æ–¯é…ç½®
    base_config::BayesianOptimizationConfig
    
    # GPUå¹¶è¡Œé…ç½®
    gpu_config::GPUParallelConfig = default_gpu_config()
    
    # æ‰¹é‡è¯„ä¼°é…ç½®
    batch_evaluation::Bool = true          # å¯ç”¨æ‰¹é‡è¯„ä¼°
    min_batch_size::Int = 10              # æœ€å°æ‰¹æ¬¡å¤§å°
    max_batch_size::Int = 100             # æœ€å¤§æ‰¹æ¬¡å¤§å°
    adaptive_batching::Bool = true         # è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°
    
    # å†…å­˜ç®¡ç†
    gpu_memory_threshold::Float64 = 0.8   # GPUå†…å­˜ä½¿ç”¨é˜ˆå€¼
    auto_memory_management::Bool = true    # è‡ªåŠ¨å†…å­˜ç®¡ç†
    
    # æ€§èƒ½ç›‘æ§
    profile_gpu_performance::Bool = false  # GPUæ€§èƒ½åˆ†æ
    track_memory_usage::Bool = true       # å†…å­˜ä½¿ç”¨è·Ÿè¸ª
    
    # å®¹é”™é…ç½®
    gpu_fallback_enabled::Bool = true     # GPUå¤±è´¥æ—¶CPUå›é€€
    max_gpu_retries::Int = 3              # GPUé‡è¯•æ¬¡æ•°
end

"""
    default_gpu_bayesian_config(base_config::BayesianOptimizationConfig)

åˆ›å»ºé»˜è®¤GPUè´å¶æ–¯é…ç½®
"""
function default_gpu_bayesian_config(base_config::BayesianOptimizationConfig)
    gpu_config = default_gpu_config()
    # é’ˆå¯¹è´å¶æ–¯ä¼˜åŒ–è°ƒæ•´GPUé…ç½®
    gpu_config = GPUParallelConfig(
        gpu_config.use_multi_gpu,
        min(2000, 500),  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
        0.7,             # ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
        :GPUTsit5,
        1e-6, 1e-3, 10000,
        false,           # åŒæ­¥å¤„ç†æ›´ç¨³å®š
        true,
        false,           # å‡å°‘è°ƒè¯•è¾“å‡º
        false
    )
    
    return GPUBayesianConfig(
        base_config = base_config,
        gpu_config = gpu_config,
        batch_evaluation = true,
        min_batch_size = 10,
        max_batch_size = 100,
        adaptive_batching = true,
        gpu_memory_threshold = 0.7,
        auto_memory_management = true,
        profile_gpu_performance = false,
        track_memory_usage = true,
        gpu_fallback_enabled = true,
        max_gpu_retries = 3
    )
end

"""
    GPUBayesianOptimizer

GPUåŠ é€Ÿçš„è´å¶æ–¯ä¼˜åŒ–å™¨
"""
mutable struct GPUBayesianOptimizer
    # ç»§æ‰¿åŸºç¡€ä¼˜åŒ–å™¨ç»„ä»¶
    base_optimizer::BayesianOptimizer
    gpu_config::GPUBayesianConfig
    
    # GPUæ±‚è§£å™¨
    gpu_solver::Union{Nothing, OptimizedGPUSolver}
    
    # æ‰¹é‡ç®¡ç†
    current_batch_size::Int
    batch_history::Vector{Int}
    
    # æ€§èƒ½ç›‘æ§
    gpu_evaluation_times::Vector{Float64}
    cpu_evaluation_times::Vector{Float64}
    memory_usage_history::Vector{Float64}
    
    # é”™è¯¯å¤„ç†
    gpu_failure_count::Int
    fallback_mode::Bool
    consecutive_invalid_batches::Int
    
    function GPUBayesianOptimizer(gpu_config::GPUBayesianConfig, param_space::ParameterSpace)
        # åˆ›å»ºåŸºç¡€ä¼˜åŒ–å™¨
        base_optimizer = BayesianOptimizer(gpu_config.base_config, param_space)
        
        optimizer = new()
        optimizer.base_optimizer = base_optimizer
        optimizer.gpu_config = gpu_config
        optimizer.gpu_solver = nothing
        optimizer.current_batch_size = gpu_config.min_batch_size
        optimizer.batch_history = Int[]
        optimizer.gpu_evaluation_times = Float64[]
        optimizer.cpu_evaluation_times = Float64[]
        optimizer.memory_usage_history = Float64[]
        optimizer.gpu_failure_count = 0
        optimizer.fallback_mode = false
        optimizer.consecutive_invalid_batches = 0
        
        # åˆå§‹åŒ–GPUæ±‚è§£å™¨
        initialize_gpu_solver!(optimizer)
        
        return optimizer
    end
end

"""
    fit_gp_with_filtered_data!(opt::GPUBayesianOptimizer)

åœ¨æ‹ŸåˆGPå‰ï¼šå»é‡ + è¿‡æ»¤æƒ©ç½š/éæœ‰é™æ ·æœ¬ï¼Œä»…ç”¨æœ‰æ•ˆæ•°æ®è®­ç»ƒï¼›æ‹Ÿåˆåæ¢å¤å®Œæ•´æ•°æ®ã€‚
"""
function fit_gp_with_filtered_data!(opt::GPUBayesianOptimizer)
    base = opt.base_optimizer
    # å…ˆå»é‡
    deduplicate_training_data!(base)
    # å¤‡ä»½
    X_full = base.X_evaluated
    y_full = base.y_evaluated
    # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬
    penalty = opt.gpu_config.base_config.constraint_penalty
    valid_idx = findall(i -> isfinite(y_full[i]) && y_full[i] != penalty, 1:length(y_full))
    if !isempty(valid_idx)
        base.X_evaluated = X_full[valid_idx, :]
        y_valid = y_full[valid_idx]
        # é’ˆå¯¹ç›®æ ‡åšç¨³å¥æ ‡å‡†åŒ–ï¼Œæå‡GPæ‹Ÿåˆç¨³å®šæ€§
        Î¼ = mean(y_valid)
        Ïƒ = std(y_valid)
        if !(isfinite(Ïƒ) && Ïƒ > 1e-8)
            Ïƒ = 1.0
        end
        y_std = (y_valid .- Î¼) ./ Ïƒ
        # å¾®å°æŠ–åŠ¨ï¼Œé¿å…å¥‡å¼‚
        y_std .+= (1e-6 .* randn(length(y_std)))
        base.y_evaluated = y_std
    else
        # æ— æœ‰æ•ˆæ ·æœ¬ï¼Œç›´æ¥ç”¨å…¨éƒ¨æ•°æ®æ‹Ÿåˆï¼ˆå¯èƒ½ä¸ºç©ºç”±ä¸‹æ¸¸å¤„ç†ï¼‰
        base.X_evaluated = X_full
        base.y_evaluated = y_full
    end
    # æ‹Ÿåˆ
    gp_model = fit_gp_model(base)
    opt.base_optimizer.gp_model = gp_model
    # æ¢å¤å®Œæ•´æ•°æ®
    base.X_evaluated = X_full
    base.y_evaluated = y_full
end

"""
    initialize_gpu_solver!(optimizer::GPUBayesianOptimizer)

åˆå§‹åŒ–GPUæ±‚è§£å™¨
"""
function initialize_gpu_solver!(optimizer::GPUBayesianOptimizer)
    try
        if CUDA.functional()
            println("ğŸš€ åˆå§‹åŒ–GPUè´å¶æ–¯ä¼˜åŒ–å™¨...")
            optimizer.gpu_solver = OptimizedGPUSolver(optimizer.gpu_config.gpu_config)
            println("âœ… GPUæ±‚è§£å™¨åˆå§‹åŒ–æˆåŠŸ")
        else
            println("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå¯ç”¨CPUå›é€€æ¨¡å¼")
            optimizer.fallback_mode = true
        end
    catch e
        println("âŒ GPUåˆå§‹åŒ–å¤±è´¥: $e")
        println("ğŸ”„ å¯ç”¨CPUå›é€€æ¨¡å¼")
        optimizer.fallback_mode = true
        optimizer.gpu_failure_count += 1
    end
end

"""
    create_gpu_objective_function(optimizer::GPUBayesianOptimizer)

åˆ›å»ºGPUåŠ é€Ÿçš„ç›®æ ‡å‡½æ•°ï¼ˆæ”¯æŒæ‰¹é‡è¯„ä¼°ï¼‰
"""
function create_gpu_objective_function(optimizer::GPUBayesianOptimizer)
    config = optimizer.gpu_config.base_config
    param_space = optimizer.base_optimizer.param_space
    
    # å•ç‚¹è¯„ä¼°å‡½æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
    function objective_single(x::Vector{Float64})
        return evaluate_batch_gpu([x], optimizer)[1]
    end
    
    # æ‰¹é‡è¯„ä¼°å‡½æ•°ï¼ˆæ–°å¢ï¼‰
    function objective_batch(X::Matrix{Float64})
        return evaluate_batch_gpu(X, optimizer)
    end
    
    # è¿”å›åŒ…å«ä¸¤ç§æ¥å£çš„å‡½æ•°å¯¹è±¡
    return (single=objective_single, batch=objective_batch)
end

"""
    evaluate_batch_gpu(X::Matrix{Float64}, optimizer::GPUBayesianOptimizer)

GPUæ‰¹é‡è¯„ä¼°ç›®æ ‡å‡½æ•°
"""
function evaluate_batch_gpu(X::Matrix{Float64}, optimizer::GPUBayesianOptimizer)
    if size(X, 1) == 0
        return Float64[]
    end
    
    config = optimizer.gpu_config.base_config
    param_space = optimizer.base_optimizer.param_space
    n_samples = size(X, 1)
    
    start_time = time()
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨GPU
    if optimizer.fallback_mode || optimizer.gpu_solver === nothing
        return evaluate_batch_cpu(X, optimizer)
    end
    
    try
        # è‡ªé€‚åº”æ‰¹æ¬¡å¤§å°ç®¡ç†
        if optimizer.gpu_config.adaptive_batching
            optimizer.current_batch_size = calculate_optimal_gpu_batch_size(optimizer, n_samples)
        end
        
        # æ„å»ºæ‰©å±•çš„å‚æ•°çŸ©é˜µï¼ˆåŒ…å«åˆå§‹æ¡ä»¶ï¼‰
        X_extended = prepare_extended_parameter_matrix(X, config, param_space)
        
        # GPUæ‰¹é‡æ±‚è§£
        target_vars = [config.target_variable]
        tspan = param_space.tspan
        
        if optimizer.gpu_config.track_memory_usage
            try
                initial_memory = CUDA.available_memory()
            catch
                initial_memory = 0
            end
        end
        
        # ä½¿ç”¨GPUæ±‚è§£å™¨è¿›è¡Œæ‰¹é‡è®¡ç®—
        results = solve_batch_gpu_optimized(
            optimizer.gpu_solver, 
            X_extended, 
            tspan, 
            target_vars
        )
        
        # æå–ç›®æ ‡å€¼
        objective_values = results[:, 1]  # ç¬¬ä¸€åˆ—æ˜¯ç›®æ ‡å˜é‡
        
        # åº”ç”¨çº¦æŸå’Œä¼˜åŒ–æ–¹å‘
        objective_values = process_objective_values(objective_values, X, config, param_space)
        
        # è®°å½•æ€§èƒ½
        evaluation_time = time() - start_time
        push!(optimizer.gpu_evaluation_times, evaluation_time)
        push!(optimizer.batch_history, n_samples)
        
        if optimizer.gpu_config.track_memory_usage
            try
                final_memory = CUDA.available_memory()
                memory_used = (initial_memory - final_memory) / 1e9  # GB
                push!(optimizer.memory_usage_history, memory_used)
            catch
                # å¿½ç•¥å†…å­˜ç»Ÿè®¡å¤±è´¥
            end
        end
        
        if optimizer.gpu_config.gpu_config.verbose
            throughput = n_samples / evaluation_time
            println("ğŸš€ GPUæ‰¹é‡è¯„ä¼°: $(n_samples)æ ·æœ¬, $(round(evaluation_time, digits=2))s, $(round(throughput, digits=1))æ ·æœ¬/ç§’")
        end
        
        return objective_values
        
    catch e
        println("âš ï¸  GPUè¯„ä¼°å¤±è´¥: $e")
        optimizer.gpu_failure_count += 1
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ‡æ¢åˆ°CPUå›é€€
        if optimizer.gpu_failure_count >= optimizer.gpu_config.max_gpu_retries
            println("ğŸ”„ GPUå¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼")
            optimizer.fallback_mode = true
        end
        
        return evaluate_batch_cpu(X, optimizer)
    end
end

"""
    evaluate_batch_cpu(X::Matrix{Float64}, optimizer::GPUBayesianOptimizer)

CPUå›é€€æ‰¹é‡è¯„ä¼°
"""
function evaluate_batch_cpu(X::Matrix{Float64}, optimizer::GPUBayesianOptimizer)
    config = optimizer.gpu_config.base_config
    param_space = optimizer.base_optimizer.param_space
    n_samples = size(X, 1)
    
    start_time = time()
    
    # åˆ›å»ºæ ‡å‡†çš„å•ç‚¹ç›®æ ‡å‡½æ•°
    standard_objective = create_objective_function(optimizer.base_optimizer)
    
    # ä¸²è¡Œè¯„ä¼°æ¯ä¸ªç‚¹
    objective_values = zeros(Float64, n_samples)
    for i in 1:n_samples
        objective_values[i] = standard_objective(X[i, :])
    end
    
    # è®°å½•CPUæ€§èƒ½
    evaluation_time = time() - start_time
    push!(optimizer.cpu_evaluation_times, evaluation_time)
    
    if optimizer.gpu_config.gpu_config.verbose
        throughput = n_samples / evaluation_time
        println("ğŸ’» CPUæ‰¹é‡è¯„ä¼°: $(n_samples)æ ·æœ¬, $(round(evaluation_time, digits=2))s, $(round(throughput, digits=1))æ ·æœ¬/ç§’")
    end
    
    return objective_values
end

"""
    prepare_extended_parameter_matrix(X::Matrix{Float64}, config, param_space)

å‡†å¤‡æ‰©å±•çš„å‚æ•°çŸ©é˜µï¼ˆåŒ…å«ååº”å¸¸æ•°å’Œåˆå§‹æ¡ä»¶ï¼‰
"""
function prepare_extended_parameter_matrix(X::Matrix{Float64}, config, param_space)
    n_samples = size(X, 1)
    
    # å‚æ•°é¡ºåºï¼š[k1f, k1r, k2f, k2r, k3f, k3r, k4f, k4r, A, B, C, E1, E2]
    # X åº”è¯¥åŒ…å«è¿™13ä¸ªå‚æ•°
    
    if size(X, 2) != 13
        error("å‚æ•°çŸ©é˜µXåº”è¯¥æœ‰13åˆ—: [k1f, k1r, k2f, k2r, k3f, k3r, k4f, k4r, A, B, C, E1, E2]")
    end
    
    return X  # ç›´æ¥è¿”å›ï¼Œå› ä¸ºXå·²ç»åŒ…å«æ‰€æœ‰å¿…è¦å‚æ•°
end

"""
    process_objective_values(values::Vector{Float64}, X::Matrix{Float64}, config, param_space)

å¤„ç†ç›®æ ‡å€¼ï¼ˆåº”ç”¨çº¦æŸã€ä¼˜åŒ–æ–¹å‘ç­‰ï¼‰
"""
function process_objective_values(values::Vector{Float64}, X::Matrix{Float64}, config, param_space)
    n_samples = length(values)
    processed_values = copy(values)
    
    for i in 1:n_samples
        # æ£€æŸ¥çº¦æŸ
        if config.apply_constraints
            if !check_parameter_constraints(X[i, :], param_space, config)
                processed_values[i] = config.constraint_penalty
                continue
            end
        end
        
        # æ£€æŸ¥æœ‰æ•ˆæ€§
        if !isfinite(processed_values[i])
            processed_values[i] = config.constraint_penalty
            continue
        end
        
        # åº”ç”¨ä¼˜åŒ–æ–¹å‘
        if config.optimization_direction == :minimize
            processed_values[i] = -processed_values[i]
        end
    end
    
    return processed_values
end

"""
    calculate_optimal_gpu_batch_size(optimizer::GPUBayesianOptimizer, n_samples::Int)

è®¡ç®—æœ€ä¼˜GPUæ‰¹æ¬¡å¤§å°
"""
function calculate_optimal_gpu_batch_size(optimizer::GPUBayesianOptimizer, n_samples::Int)
    config = optimizer.gpu_config
    
    # åŸºäºGPUå†…å­˜çŠ¶æ€è°ƒæ•´
    if config.auto_memory_management && CUDA.functional()
        try
            free_mem = CUDA.available_memory()
            total_mem = CUDA.totalmem(CUDA.device())
            memory_ratio = free_mem / total_mem
            if memory_ratio < config.gpu_memory_threshold
                new_batch_size = max(config.min_batch_size, optimizer.current_batch_size Ã· 2)
            else
                new_batch_size = min(config.max_batch_size, optimizer.current_batch_size * 2)
            end
        catch
            new_batch_size = optimizer.current_batch_size
        end
    else
        new_batch_size = optimizer.current_batch_size
    end
    
    # ä¸è¶…è¿‡å¾…è¯„ä¼°æ ·æœ¬æ•°
    return min(new_batch_size, n_samples, config.max_batch_size)
end

"""
    run_gpu_bayesian_optimization!(optimizer::GPUBayesianOptimizer)

è¿è¡ŒGPUåŠ é€Ÿçš„è´å¶æ–¯ä¼˜åŒ–
"""
function run_gpu_bayesian_optimization!(optimizer::GPUBayesianOptimizer)
    base_config = optimizer.gpu_config.base_config
    gpu_config = optimizer.gpu_config
    
    println("ğŸš€ å¼€å§‹GPUåŠ é€Ÿè´å¶æ–¯ä¼˜åŒ–...")
    println("ğŸ¯ ä¼˜åŒ–ç›®æ ‡: $(base_config.target_variable)")
    println("ğŸ”„ è¿­ä»£æ¬¡æ•°: $(base_config.n_iterations)")
    println("ğŸ’¾ GPUæ¨¡å¼: $(optimizer.fallback_mode ? "CPUå›é€€" : "GPUåŠ é€Ÿ")")
    
    # åˆå§‹åŒ–åŸºç¡€ä¼˜åŒ–å™¨
    initialize_optimizer!(optimizer.base_optimizer)
    
    # åˆ›å»ºGPUç›®æ ‡å‡½æ•°
    objective_functions = create_gpu_objective_function(optimizer)
    
    # ä¼˜åŒ–ä¸»å¾ªç¯
    for iter in 1:base_config.n_iterations
        println("\n--- GPUè¿­ä»£ $iter/$(base_config.n_iterations) ---")
        
        # 1. æ‹Ÿåˆå‰ï¼šå»é‡ + æœ‰æ•ˆæ ·æœ¬è¿‡æ»¤ï¼Œæé«˜é‡‡é›†å‡½æ•°ç¨³å®šæ€§
        fit_gp_with_filtered_data!(optimizer)
        
        # 2. ä¼˜åŒ–é‡‡é›†å‡½æ•°ï¼ˆæ‰¹é‡å€™é€‰ç‚¹ç”Ÿæˆï¼‰
        candidate_points = generate_candidate_batch(optimizer)
        
        if isempty(candidate_points)
            println("âš ï¸  å€™é€‰ç‚¹ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡æ­¤æ¬¡è¿­ä»£")
            continue
        end
        
        # 3. æ‰¹é‡è¯„ä¼°å€™é€‰ç‚¹
        batch_size = size(candidate_points, 1)
        println("ğŸ“Š æ‰¹é‡è¯„ä¼° $(batch_size) ä¸ªå€™é€‰ç‚¹...")
        
        candidate_values = evaluate_batch_gpu(candidate_points, optimizer)
        # å¦‚æœGPUæ‰¹é‡å…¨éƒ¨ä¸ºæ— æ•ˆ/æƒ©ç½šå€¼ï¼Œå°è¯•CPUæ•´ä½“å›é€€
        if all(v -> !isfinite(v) || v == optimizer.gpu_config.base_config.constraint_penalty, candidate_values)
            if !optimizer.fallback_mode
                println("âš ï¸  GPUæ‰¹é‡è¯„ä¼°å¾—åˆ°å…¨æ— æ•ˆå€¼ï¼Œä¸´æ—¶ä½¿ç”¨CPUè¯„ä¼°å›é€€")
            end
            # ç»Ÿè®¡ä¸ºä¸€æ¬¡GPUå¤±è´¥ï¼Œå¹¶è€ƒè™‘åˆ‡æ¢åˆ°CPUæ¨¡å¼
            optimizer.gpu_failure_count += 1
            optimizer.consecutive_invalid_batches += 1
            if optimizer.gpu_failure_count >= optimizer.gpu_config.max_gpu_retries
                optimizer.fallback_mode = true
            end
            # è¿ç»­å¤šæ¬¡æ— æ•ˆæ—¶ï¼Œç¼©å°æ‰¹æ¬¡å¹¶å›é€€CPUè¯„ä¼°
            if optimizer.consecutive_invalid_batches >= 2
                optimizer.current_batch_size = max(optimizer.gpu_config.min_batch_size, optimizer.current_batch_size Ã· 2)
                optimizer.fallback_mode = true
            end
            candidate_values = evaluate_batch_cpu(candidate_points, optimizer)
        else
            optimizer.consecutive_invalid_batches = 0
            # å¯¹æ— æ•ˆå­é›†æ··åˆé‡è¯„ï¼šä»…å¯¹æƒ©ç½š/éæœ‰é™å…ƒç´ ç”¨CPUé‡è¯„
            penalty = optimizer.gpu_config.base_config.constraint_penalty
            for i in eachindex(candidate_values)
                if !isfinite(candidate_values[i]) || candidate_values[i] == penalty
                    candidate_values[i] = evaluate_batch_cpu(candidate_points[i:i, :], optimizer)[1]
                end
            end
        end
        
        # 4. é€‰æ‹©æœ€ä½³å€™é€‰ç‚¹
        best_candidate_idx = argmax(candidate_values)
        next_x = candidate_points[best_candidate_idx, :]
        next_y = candidate_values[best_candidate_idx]
        
        # 5. æ›´æ–°å†å²è®°å½•
        optimizer.base_optimizer.X_evaluated = vcat(optimizer.base_optimizer.X_evaluated, next_x')
        optimizer.base_optimizer.y_evaluated = vcat(optimizer.base_optimizer.y_evaluated, next_y)
        
        # 6. æ›´æ–°æœ€ä¼˜ç»“æœ
        if next_y > optimizer.base_optimizer.best_y
            optimizer.base_optimizer.best_x = next_x
            optimizer.base_optimizer.best_y = next_y
            optimizer.base_optimizer.best_params = vector_to_params_dict(next_x, optimizer.base_optimizer.param_space)
            println("ğŸ‰ å‘ç°æ–°çš„æœ€ä¼˜ç‚¹! GPUç›®æ ‡å€¼: $(round(next_y, digits=4))")
        else
            println("ğŸ“Š å½“å‰ç‚¹GPUç›®æ ‡å€¼: $(round(next_y, digits=4))")
        end
        
        # 7. è®°å½•é‡‡é›†å‡½æ•°å€¼
        acquisition_value = evaluate_acquisition_function(optimizer.base_optimizer, next_x)
        optimizer.base_optimizer.acquisition_history = vcat(optimizer.base_optimizer.acquisition_history, acquisition_value)
        
        # 8. æ€§èƒ½ç›‘æ§å’Œè°ƒæ•´
        if gpu_config.adaptive_batching
            update_batch_size_strategy(optimizer)
        end
        
        # 9. ä¸­é—´ç»“æœä¿å­˜
        if base_config.save_intermediate && iter % 10 == 0
            save_gpu_intermediate_results(optimizer, iter)
        end
        
        # 10. å†…å­˜æ¸…ç†
        if gpu_config.auto_memory_management && !optimizer.fallback_mode
            if iter % 5 == 0
                CUDA.reclaim()
            end
        end
    end
    
    println("\nğŸ‰ GPUè´å¶æ–¯ä¼˜åŒ–å®Œæˆ!")
    analyze_gpu_performance(optimizer)
    
    return optimizer
end

"""
    generate_candidate_batch(optimizer::GPUBayesianOptimizer)

ç”Ÿæˆå€™é€‰ç‚¹æ‰¹æ¬¡ï¼ˆæ”¹è¿›çš„é‡‡é›†å‡½æ•°ä¼˜åŒ–ï¼‰
"""
function generate_candidate_batch(optimizer::GPUBayesianOptimizer)
    config = optimizer.gpu_config
    param_space = optimizer.base_optimizer.param_space
    
    # æœŸæœ›æ‰¹æ¬¡å¤§å°ï¼ˆé’ˆå¯¹é‡‡é›†å‡½æ•°å»ºè®®å°æ‰¹æ¬¡ï¼Œä½†è¦ç¡®ä¿æ•°é‡è¶³å¤Ÿï¼‰
    target_batch = min(config.max_batch_size, 20)
    min_batch = max(10, config.min_batch_size)
    n_dims = length(get_parameter_ranges(param_space))
    
    # æ”¶é›†å€™é€‰ï¼ˆæ··åˆç­–ç•¥ï¼šé‡‡é›†å‡½æ•° + éšæœº + æœ€ä¼˜ç‚¹é™„è¿‘æ‰°åŠ¨ï¼‰
    seen = Set{String}()
    collected = Vector{Vector{Float64}}()
    
    # 1) é‡‡é›†å‡½æ•°å°è¯•ï¼ˆé™æ¬¡ï¼Œé¿å…åˆ·å±ï¼‰
    acq_trials = min(5, target_batch)
    for _ in 1:acq_trials
        if length(collected) >= target_batch
            break
        end
        candidate = robust_optimize_acquisition(optimizer.base_optimizer; n_starts=6)
        if candidate === nothing
            continue
        end
        k = _row_key(candidate)
        if !(k in seen) && check_parameter_constraints(candidate, param_space, config.base_config)
            push!(seen, k)
            push!(collected, candidate)
        end
    end
    
    # 2) æœ€ä¼˜ç‚¹é™„è¿‘çš„å±€éƒ¨æ‰°åŠ¨ï¼ˆæ¢ç´¢-å¼€å‘æ··åˆï¼‰
    if length(collected) < target_batch && optimizer.base_optimizer.best_x !== nothing
        best_x = optimizer.base_optimizer.best_x
        # æ‰°åŠ¨å°ºåº¦ï¼šæŒ‰å‚æ•°èŒƒå›´çš„ 5%
        ranges = get_parameter_ranges(param_space)
        sigma = [0.05 * (maximum(r) - minimum(r)) for r in ranges]
        local_attempts = 0
        while length(collected) < target_batch && local_attempts < target_batch * 5
            local_attempts += 1
            x = similar(best_x)
            for j in 1:n_dims
                x[j] = best_x[j] + randn() * sigma[j]
                # æŠ•å½±å›è¾¹ç•Œ
                lo, hi = minimum(ranges[j]), maximum(ranges[j])
                if x[j] < lo
                    x[j] = lo + abs(x[j] - lo)
                elseif x[j] > hi
                    x[j] = hi - abs(x[j] - hi)
                end
                x[j] = clamp(x[j], lo, hi)
            end
            if check_parameter_constraints(x, param_space, config.base_config)
                k = _row_key(x)
                if !(k in seen)
                    push!(seen, k)
                    push!(collected, x)
                end
            end
        end
    end

    # 2b) å‰Kä¸ªå†å²æœ€ä¼˜çš„é‚»åŸŸæ‰°åŠ¨ï¼ˆå¢å¼ºå¼€å‘èƒ½åŠ›ï¼‰
    if length(collected) < target_batch
        K = min(5, size(optimizer.base_optimizer.X_evaluated, 1))
        if K > 0
            # é€‰å–å‰Kå¤§yå¯¹åº”çš„X
            y_hist = optimizer.base_optimizer.y_evaluated
            idx_sorted = sortperm(y_hist, rev=true)
            ranges = get_parameter_ranges(param_space)
            sigma2 = [0.08 * (maximum(r) - minimum(r)) for r in ranges]
            added = 0
            for kidx in idx_sorted[1:K]
                center = vec(optimizer.base_optimizer.X_evaluated[kidx, :])
                for _ in 1:3
                    x = copy(center)
                    for j in 1:n_dims
                        x[j] = center[j] + randn() * sigma2[j]
                        lo, hi = minimum(ranges[j]), maximum(ranges[j])
                        x[j] = clamp(x[j], lo, hi)
                    end
                    if check_parameter_constraints(x, param_space, config.base_config)
                        k = _row_key(x)
                        if !(k in seen)
                            push!(seen, k)
                            push!(collected, x)
                            added += 1
                            if length(collected) >= target_batch
                                break
                            end
                        end
                    end
                end
                if length(collected) >= target_batch
                    break
                end
            end
        end
    end
    
    # 3) éšæœºè¡¥é½ï¼ˆä¼˜å…ˆç”¨LHSï¼Œä¿è¯è¦†ç›–åº¦ï¼‰
    if length(collected) < target_batch
        ranges = get_parameter_ranges(param_space)
        need = target_batch - length(collected)
        if need > 0
            Xlhs = _lhs_sample(need, ranges)
            for i in 1:size(Xlhs, 1)
                x = vec(Xlhs[i, :])
                if check_parameter_constraints(x, param_space, config.base_config)
                    k = _row_key(x)
                    if !(k in seen)
                        push!(seen, k)
                        push!(collected, x)
                    end
                end
            end
        end
        # è‹¥ä»ä¸è¶³ï¼Œå°‘é‡å‡åŒ€éšæœºè¡¥é½
        random_attempts = 0
        while length(collected) < target_batch && random_attempts < target_batch * 10
            random_attempts += 1
            x = zeros(Float64, n_dims)
            for j in 1:n_dims
                lo, hi = minimum(ranges[j]), maximum(ranges[j])
                x[j] = lo + rand() * (hi - lo)
            end
            if check_parameter_constraints(x, param_space, config.base_config)
                k = _row_key(x)
                if !(k in seen)
                    push!(seen, k)
                    push!(collected, x)
                end
            end
        end
        # è‹¥é‡‡é›†ä¼˜åŒ–æœªè·å¾—å€™é€‰ï¼Œé™é»˜å›é€€ä¸ºéšæœº/LHSï¼Œä¸é‡å¤æ‰“å°
    end
    
    if isempty(collected)
        return Matrix{Float64}(undef, 0, 0)
    end
    
    # è‹¥ä»ä¸è¶³æœ€å°æ‰¹æ¬¡ï¼Œå°½åŠ›è¡¥é½ï¼ˆå†å¤šå°è¯•ä¸€äº›éšæœºç‚¹ï¼‰
    if length(collected) < min_batch
        ranges = get_parameter_ranges(param_space)
        need = min_batch - length(collected)
        Xlhs2 = _lhs_sample(need, ranges)
        for i in 1:size(Xlhs2, 1)
            x = vec(Xlhs2[i, :])
            if check_parameter_constraints(x, param_space, config.base_config)
                k = _row_key(x)
                if !(k in seen)
                    push!(seen, k)
                    push!(collected, x)
                end
            end
        end
    end
    
    # è¾“å‡ºçŸ©é˜µ
    candidate_matrix = zeros(Float64, length(collected), n_dims)
    for (i, x) in enumerate(collected)
        candidate_matrix[i, :] = x
    end
    return candidate_matrix
end

"""
    update_batch_size_strategy(optimizer::GPUBayesianOptimizer)

æ›´æ–°æ‰¹æ¬¡å¤§å°ç­–ç•¥
"""
function update_batch_size_strategy(optimizer::GPUBayesianOptimizer)
    config = optimizer.gpu_config
    
    # åŸºäºæœ€è¿‘çš„æ€§èƒ½è°ƒæ•´æ‰¹æ¬¡å¤§å°
    if length(optimizer.gpu_evaluation_times) >= 3
        recent_times = optimizer.gpu_evaluation_times[end-2:end]
        recent_batches = optimizer.batch_history[end-2:end]
        
        # è®¡ç®—å¹³å‡ååé‡
        avg_throughput = mean(recent_batches ./ recent_times)
        
        # åŠ¨æ€è°ƒæ•´ç­–ç•¥
        if avg_throughput > 50  # ååé‡è¾ƒé«˜ï¼Œå¯ä»¥å¢å¤§æ‰¹æ¬¡
            optimizer.current_batch_size = min(config.max_batch_size, 
                                              Int(round(optimizer.current_batch_size * 1.2)))
        elseif avg_throughput < 10  # ååé‡è¾ƒä½ï¼Œå‡å°æ‰¹æ¬¡
            optimizer.current_batch_size = max(config.min_batch_size,
                                              Int(round(optimizer.current_batch_size * 0.8)))
        end
    end
end

"""
    analyze_gpu_performance(optimizer::GPUBayesianOptimizer)

åˆ†æGPUæ€§èƒ½è¡¨ç°
"""
function analyze_gpu_performance(optimizer::GPUBayesianOptimizer)
    println("\nğŸ“Š GPUæ€§èƒ½åˆ†æ:")
    
    # GPU vs CPUæ€§èƒ½å¯¹æ¯”
    if !isempty(optimizer.gpu_evaluation_times) && !isempty(optimizer.cpu_evaluation_times)
        gpu_avg_time = mean(optimizer.gpu_evaluation_times)
        cpu_avg_time = mean(optimizer.cpu_evaluation_times)
        speedup = cpu_avg_time / gpu_avg_time
        
        println("  GPUå¹³å‡è¯„ä¼°æ—¶é—´: $(round(gpu_avg_time, digits=3))s")
        println("  CPUå¹³å‡è¯„ä¼°æ—¶é—´: $(round(cpu_avg_time, digits=3))s")
        println("  GPUåŠ é€Ÿæ¯”: $(round(speedup, digits=1))x")
    elseif !isempty(optimizer.gpu_evaluation_times)
        gpu_avg_time = mean(optimizer.gpu_evaluation_times)
        total_evaluations = sum(optimizer.batch_history)
        total_time = sum(optimizer.gpu_evaluation_times)
        avg_throughput = total_evaluations / total_time
        
        println("  GPUæ€»è¯„ä¼°æ—¶é—´: $(round(total_time, digits=2))s")
        println("  GPUæ€»è¯„ä¼°æ ·æœ¬: $total_evaluations")
        println("  GPUå¹³å‡ååé‡: $(round(avg_throughput, digits=1)) æ ·æœ¬/ç§’")
    end
    
    # æ‰¹æ¬¡å¤§å°æ¼”åŒ–
    if !isempty(optimizer.batch_history)
        println("  æ‰¹æ¬¡å¤§å°èŒƒå›´: $(minimum(optimizer.batch_history)) - $(maximum(optimizer.batch_history))")
        println("  å¹³å‡æ‰¹æ¬¡å¤§å°: $(round(mean(optimizer.batch_history), digits=1))")
    end
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    if !isempty(optimizer.memory_usage_history)
        println("  å¹³å‡GPUå†…å­˜ä½¿ç”¨: $(round(mean(optimizer.memory_usage_history), digits=2)) GB")
        println("  å³°å€¼GPUå†…å­˜ä½¿ç”¨: $(round(maximum(optimizer.memory_usage_history), digits=2)) GB")
    end
    
    # é”™è¯¯ç»Ÿè®¡
    if optimizer.gpu_failure_count > 0
        println("  GPUå¤±è´¥æ¬¡æ•°: $(optimizer.gpu_failure_count)")
        println("  å½“å‰æ¨¡å¼: $(optimizer.fallback_mode ? "CPUå›é€€" : "GPUåŠ é€Ÿ")")
    end
end

"""
    save_gpu_intermediate_results(optimizer::GPUBayesianOptimizer, iteration::Int)

ä¿å­˜GPUä¼˜åŒ–ä¸­é—´ç»“æœ
"""
function save_gpu_intermediate_results(optimizer::GPUBayesianOptimizer, iteration::Int)
    results_dir = "/home/ryankwok/Documents/TwoEnzymeSim/ML/result"
    
    if !isdir(results_dir)
        mkpath(results_dir)
    end
    
    filename = joinpath(results_dir, "gpu_bayesian_opt_iter_$(iteration).jld2")
    
    try
        jldsave(filename;
                # åŸºç¡€ä¼˜åŒ–ç»“æœ
                X_evaluated = optimizer.base_optimizer.X_evaluated,
                y_evaluated = optimizer.base_optimizer.y_evaluated,
                best_x = optimizer.base_optimizer.best_x,
                best_y = optimizer.base_optimizer.best_y,
                best_params = optimizer.base_optimizer.best_params,
                
                # GPUæ€§èƒ½æ•°æ®
                gpu_evaluation_times = optimizer.gpu_evaluation_times,
                cpu_evaluation_times = optimizer.cpu_evaluation_times,
                batch_history = optimizer.batch_history,
                memory_usage_history = optimizer.memory_usage_history,
                current_batch_size = optimizer.current_batch_size,
                gpu_failure_count = optimizer.gpu_failure_count,
                fallback_mode = optimizer.fallback_mode,
                
                # é…ç½®ä¿¡æ¯
                gpu_config = optimizer.gpu_config,
                iteration = iteration)
        
        println("ğŸ’¾ GPUä¸­é—´ç»“æœå·²ä¿å­˜: $filename")
    catch e
        println("âš ï¸  ä¿å­˜GPUä¸­é—´ç»“æœå¤±è´¥: $e")
    end
end

"""
    create_gpu_bayesian_workflow(config_path::String="config/bayesian_optimization_config.toml", section::String="single_objective")

åˆ›å»ºGPUåŠ é€Ÿè´å¶æ–¯ä¼˜åŒ–å·¥ä½œæµç¨‹
"""
function create_gpu_bayesian_workflow(config_path::String="config/bayesian_optimization_config.toml", section::String="single_objective")
    println("ğŸš€ GPUåŠ é€Ÿè´å¶æ–¯ä¼˜åŒ–å·¥ä½œæµç¨‹")
    println("="^60)
    
    # ç¬¬1æ­¥ï¼šåŠ è½½é…ç½®
    println("\nğŸ“‹ ç¬¬1æ­¥ï¼šåŠ è½½é…ç½®")
    base_config = load_bayesian_config(config_path, section)
    param_space = load_parameter_space_from_config(config_path)
    
    # åˆ›å»ºGPUé…ç½®
    gpu_config = default_gpu_bayesian_config(base_config)
    
    println("âœ… GPUè´å¶æ–¯ä¼˜åŒ–é…ç½®å®Œæˆ")
    println("ğŸ¯ ä¼˜åŒ–ç›®æ ‡: $(base_config.target_variable)")
    println("ğŸ” åˆå§‹ç‚¹æ•°: $(base_config.n_initial_points)")
    println("ğŸ”„ ä¼˜åŒ–è¿­ä»£: $(base_config.n_iterations)")
    println("ğŸ’¾ GPUæ¨¡å¼: $(CUDA.functional() ? "CUDAå¯ç”¨" : "CPUå›é€€")")
    println("ğŸ“Š æ‰¹é‡å¤§å°: $(gpu_config.min_batch_size)-$(gpu_config.max_batch_size)")
    
    # ç¬¬2æ­¥ï¼šåˆ›å»ºGPUä¼˜åŒ–å™¨
    println("\nğŸ—ï¸  ç¬¬2æ­¥ï¼šåˆ›å»ºGPUè´å¶æ–¯ä¼˜åŒ–å™¨")
    optimizer = GPUBayesianOptimizer(gpu_config, param_space)
    
    # ç¬¬3æ­¥ï¼šè¿è¡ŒGPUä¼˜åŒ–
    println("\nğŸ¯ ç¬¬3æ­¥ï¼šGPUæ™ºèƒ½å‚æ•°ä¼˜åŒ–")
    run_gpu_bayesian_optimization!(optimizer)
    
    # ç¬¬4æ­¥ï¼šç»“æœåˆ†æ
    println("\nğŸ“Š ç¬¬4æ­¥ï¼šç»“æœåˆ†æ")
    analyze_optimization_results(optimizer.base_optimizer)
    analyze_gpu_performance(optimizer)
    
    # ç¬¬5æ­¥ï¼šå¯è§†åŒ–ç»“æœ
    println("\nğŸ“ˆ ç¬¬5æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–")
    if base_config.plot_convergence
        plot_gpu_optimization_convergence(optimizer)
    end
    
    # ç¬¬6æ­¥ï¼šä¿å­˜æœ€ç»ˆç»“æœ
    println("\nğŸ’¾ ç¬¬6æ­¥ï¼šä¿å­˜GPUä¼˜åŒ–ç»“æœ")
    save_gpu_optimization_results(optimizer)
    
    # ç¬¬7æ­¥ï¼šæ€§èƒ½å¯¹æ¯”
    println("\nâš¡ ç¬¬7æ­¥ï¼šGPUæ•ˆç‡åˆ†æ")
    compare_gpu_efficiency(optimizer)
    
    println("\nğŸ‰ GPUè´å¶æ–¯ä¼˜åŒ–å·¥ä½œæµç¨‹å®Œæˆ!")
    
    return optimizer
end

"""
    plot_gpu_optimization_convergence(optimizer::GPUBayesianOptimizer)

ç»˜åˆ¶GPUä¼˜åŒ–æ”¶æ•›æ›²çº¿
"""
function plot_gpu_optimization_convergence(optimizer::GPUBayesianOptimizer)
    base_optimizer = optimizer.base_optimizer
    y_values = base_optimizer.y_evaluated
    n_points = length(y_values)
    
    # è®¡ç®—ç´¯ç§¯æœ€ä¼˜å€¼
    cumulative_best = zeros(n_points)
    cumulative_best[1] = y_values[1]
    
    for i in 2:n_points
        cumulative_best[i] = max(cumulative_best[i-1], y_values[i])
    end
    
    # ç»˜åˆ¶æ”¶æ•›æ›²çº¿
    p1 = plot(1:n_points, cumulative_best, 
              xlabel="è¯„ä¼°æ¬¡æ•°", ylabel="æœ€ä¼˜ç›®æ ‡å€¼", 
              title="GPUè´å¶æ–¯ä¼˜åŒ–æ”¶æ•›æ›²çº¿", 
              lw=2, label="GPUç´¯ç§¯æœ€ä¼˜å€¼", color=:blue)
    
    # æ·»åŠ åˆå§‹æ¢ç´¢é˜¶æ®µæ ‡è®°
    vline!([base_optimizer.config.n_initial_points], 
           label="åˆå§‹æ¢ç´¢ç»“æŸ", color=:red, ls=:dash)
    
    # ä¿å­˜å›¾ç‰‡
    results_dir = "/home/ryankwok/Documents/TwoEnzymeSim/ML/result"
    if !isdir(results_dir)
        mkpath(results_dir)
    end
    
    savefig(p1, joinpath(results_dir, "gpu_bayesian_convergence.png"))
    println("ğŸ“ å·²ä¿å­˜GPUæ”¶æ•›æ›²çº¿: $(joinpath(results_dir, "gpu_bayesian_convergence.png"))")
    
    # ç»˜åˆ¶GPUæ€§èƒ½æ›²çº¿
    if !isempty(optimizer.gpu_evaluation_times)
        p2 = plot(1:length(optimizer.gpu_evaluation_times), optimizer.gpu_evaluation_times,
                   xlabel="GPUè¯„ä¼°æ‰¹æ¬¡", ylabel="è¯„ä¼°æ—¶é—´ (ç§’)",
                   title="GPUæ‰¹é‡è¯„ä¼°æ€§èƒ½",
                   lw=2, label="GPUè¯„ä¼°æ—¶é—´", color=:green)
        
        # æ·»åŠ æ‰¹æ¬¡å¤§å°ä¿¡æ¯
        batch_sizes = optimizer.batch_history
        if length(batch_sizes) == length(optimizer.gpu_evaluation_times)
            throughput = batch_sizes ./ optimizer.gpu_evaluation_times
            
            p3 = plot(1:length(throughput), throughput,
                     xlabel="GPUè¯„ä¼°æ‰¹æ¬¡", ylabel="ååé‡ (æ ·æœ¬/ç§’)",
                     title="GPUè¯„ä¼°ååé‡",
                     lw=2, label="GPUååé‡", color=:purple)
            
            # ç»„åˆå›¾
            p_combined = plot(p2, p3, layout=(2,1), size=(800, 600))
            savefig(p_combined, joinpath(results_dir, "gpu_performance_curves.png"))
            println("ğŸ“ å·²ä¿å­˜GPUæ€§èƒ½æ›²çº¿: $(joinpath(results_dir, "gpu_performance_curves.png"))")
        end
    end
end

"""
    save_gpu_optimization_results(optimizer::GPUBayesianOptimizer)

ä¿å­˜GPUä¼˜åŒ–ç»“æœ
"""
function save_gpu_optimization_results(optimizer::GPUBayesianOptimizer)
    results_path = "/home/ryankwok/Documents/TwoEnzymeSim/ML/model/gpu_bayesian_optimization_results.jld2"
    
    try
        jldsave(results_path;
                # åŸºç¡€ä¼˜åŒ–ç»“æœ
                config = optimizer.base_optimizer.config,
                param_space = optimizer.base_optimizer.param_space,
                X_evaluated = optimizer.base_optimizer.X_evaluated,
                y_evaluated = optimizer.base_optimizer.y_evaluated,
                best_x = optimizer.base_optimizer.best_x,
                best_y = optimizer.base_optimizer.best_y,
                best_params = optimizer.base_optimizer.best_params,
                acquisition_history = optimizer.base_optimizer.acquisition_history,
                
                # GPUç‰¹å®šç»“æœ
                gpu_config = optimizer.gpu_config,
                gpu_evaluation_times = optimizer.gpu_evaluation_times,
                cpu_evaluation_times = optimizer.cpu_evaluation_times,
                batch_history = optimizer.batch_history,
                memory_usage_history = optimizer.memory_usage_history,
                final_batch_size = optimizer.current_batch_size,
                gpu_failure_count = optimizer.gpu_failure_count,
                final_mode = optimizer.fallback_mode ? "CPU" : "GPU")
        
        println("âœ… GPUä¼˜åŒ–ç»“æœå·²ä¿å­˜: $results_path")
        
        # ä¿å­˜æ–‡ä»¶å¤§å°ä¿¡æ¯
        file_size_mb = round(filesize(results_path) / 1024^2, digits=1)
        println("ğŸ“Š ç»“æœæ–‡ä»¶å¤§å°: $(file_size_mb) MB")
        
    catch e
        println("âŒ ä¿å­˜GPUç»“æœå¤±è´¥: $e")
    end
end

"""
    compare_gpu_efficiency(optimizer::GPUBayesianOptimizer)

GPUæ•ˆç‡å¯¹æ¯”åˆ†æ
"""
function compare_gpu_efficiency(optimizer::GPUBayesianOptimizer)
    n_evaluated = size(optimizer.base_optimizer.X_evaluated, 1)
    
    println("âš¡ GPUè´å¶æ–¯ä¼˜åŒ–æ•ˆç‡åˆ†æ:")
    println("ğŸ“Š GPUæ€»è¯„ä¼°æ¬¡æ•°: $n_evaluated")
    
    # ä¼°ç®—ä¼ ç»Ÿæ–¹æ³•çš„è®¡ç®—é‡
    param_space = optimizer.base_optimizer.param_space
    ranges = get_parameter_ranges(param_space)
    
    # ç½‘æ ¼æœç´¢ä¼°ç®—
    grid_points_coarse = 10^length(ranges)
    grid_points_fine = 20^length(ranges)
    
    println("ğŸ”² ç­‰æ•ˆç²—ç½‘æ ¼ç‚¹æ•°: $(grid_points_coarse)")
    println("ğŸ”³ ç­‰æ•ˆç»†ç½‘æ ¼ç‚¹æ•°: $(grid_points_fine)")
    
    # GPUåŠ é€Ÿæ•ˆæœ
    if !isempty(optimizer.gpu_evaluation_times) && !isempty(optimizer.cpu_evaluation_times)
        gpu_total_time = sum(optimizer.gpu_evaluation_times)
        cpu_total_time = sum(optimizer.cpu_evaluation_times)
        speedup = cpu_total_time / gpu_total_time
        
        println("\nğŸš€ GPUåŠ é€Ÿæ•ˆæœ:")
        println("  GPUæ€»è®¡ç®—æ—¶é—´: $(round(gpu_total_time, digits=2))s")
        println("  CPUæ€»è®¡ç®—æ—¶é—´: $(round(cpu_total_time, digits=2))s")
        println("  GPUåŠ é€Ÿæ¯”: $(round(speedup, digits=1))x")
        
        # ä¼°ç®—ç½‘æ ¼æœç´¢æ—¶é—´èŠ‚çœ
        estimated_grid_time_hours = grid_points_coarse * (cpu_total_time / n_evaluated) / 3600
        gpu_time_hours = gpu_total_time / 3600
        time_saving_hours = estimated_grid_time_hours - gpu_time_hours
        
        println("\nâ° æ—¶é—´èŠ‚çœä¼°ç®—:")
        println("  ä¼°ç®—ç½‘æ ¼æœç´¢æ—¶é—´: $(round(estimated_grid_time_hours, digits=1)) å°æ—¶")
        println("  GPUè´å¶æ–¯ä¼˜åŒ–æ—¶é—´: $(round(gpu_time_hours, digits=3)) å°æ—¶")
        println("  èŠ‚çœæ—¶é—´: $(round(time_saving_hours, digits=1)) å°æ—¶")
        
    elseif !isempty(optimizer.gpu_evaluation_times)
        gpu_total_time = sum(optimizer.gpu_evaluation_times)
        total_samples = sum(optimizer.batch_history)
        avg_throughput = total_samples / gpu_total_time
        
        println("\nğŸš€ GPUæ€§èƒ½ç»Ÿè®¡:")
        println("  GPUå¹³å‡ååé‡: $(round(avg_throughput, digits=1)) æ ·æœ¬/ç§’")
        println("  GPUæ€»è®¡ç®—æ—¶é—´: $(round(gpu_total_time, digits=2))s")
        
        # ä¼°ç®—ç½‘æ ¼æœç´¢æ—¶é—´
        estimated_grid_time = grid_points_coarse / avg_throughput / 3600  # å°æ—¶
        gpu_time_hours = gpu_total_time / 3600
        
        println("  ä¼°ç®—ç½‘æ ¼æœç´¢æ—¶é—´: $(round(estimated_grid_time, digits=1)) å°æ—¶")
        println("  èŠ‚çœæ—¶é—´: $(round(estimated_grid_time - gpu_time_hours, digits=1)) å°æ—¶")
    end
    
    # å†…å­˜æ•ˆç‡
    if !isempty(optimizer.memory_usage_history)
        avg_memory = mean(optimizer.memory_usage_history)
        max_memory = maximum(optimizer.memory_usage_history)
        
        println("\nğŸ’¾ GPUå†…å­˜æ•ˆç‡:")
        println("  å¹³å‡å†…å­˜ä½¿ç”¨: $(round(avg_memory, digits=2)) GB")
        println("  å³°å€¼å†…å­˜ä½¿ç”¨: $(round(max_memory, digits=2)) GB")
        
        if CUDA.functional()
            total_memory = CUDA.totalmem(CUDA.device()) / 1e9
            memory_efficiency = (avg_memory / total_memory) * 100
            println("  å†…å­˜åˆ©ç”¨ç‡: $(round(memory_efficiency, digits=1))%")
        end
    end
    
    println("\nâœ… GPUè´å¶æ–¯ä¼˜åŒ–æˆåŠŸå®ç°æ™ºèƒ½å‚æ•°æ¢ç´¢åŠ é€Ÿ!")
end

# å¯¼å‡ºä¸»è¦å‡½æ•°
export GPUBayesianConfig, GPUBayesianOptimizer
export create_gpu_bayesian_workflow, run_gpu_bayesian_optimization!
export default_gpu_bayesian_config
export analyze_gpu_performance, save_gpu_optimization_results

# ä¸»ç¨‹åºå…¥å£
if abspath(PROGRAM_FILE) == @__FILE__
    println("ğŸ¬ æ‰§è¡ŒGPUåŠ é€Ÿè´å¶æ–¯ä¼˜åŒ–å·¥ä½œæµç¨‹...")
    
    # é…ç½®æ–‡ä»¶è·¯å¾„
    config_path = "config/bayesian_optimization_config.toml"
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©å·¥ä½œæµç¨‹
    if length(ARGS) > 0
        if ARGS[1] == "--single" || ARGS[1] == "-s"
            section = length(ARGS) > 1 ? ARGS[2] : "single_objective"
            optimizer = create_gpu_bayesian_workflow(config_path, section)
            
        elseif ARGS[1] == "--config" || ARGS[1] == "-c"
            # æŒ‡å®šé…ç½®æ–‡ä»¶
            if length(ARGS) > 1
                config_path = ARGS[2]
            end
            section = length(ARGS) > 2 ? ARGS[3] : "single_objective"
            optimizer = create_gpu_bayesian_workflow(config_path, section)
            
        elseif ARGS[1] == "--help" || ARGS[1] == "-h"
            println("ğŸ“š GPUè´å¶æ–¯ä¼˜åŒ–ä½¿ç”¨è¯´æ˜:")
            println("  julia bayesian_optimization_gpu.jl                    # é»˜è®¤GPUä¼˜åŒ–")
            println("  julia bayesian_optimization_gpu.jl --single           # GPUå•ç›®æ ‡ä¼˜åŒ–")
            println("  julia bayesian_optimization_gpu.jl --config <path>    # æŒ‡å®šé…ç½®æ–‡ä»¶")
            println("  julia bayesian_optimization_gpu.jl --help             # æ˜¾ç¤ºå¸®åŠ©")
            println("\nğŸš€ GPUç‰¹æ€§:")
            println("  - æ‰¹é‡GPUå¹¶è¡Œè¯„ä¼°")
            println("  - æ™ºèƒ½å†…å­˜ç®¡ç†")
            println("  - å¤šGPUæ”¯æŒ")
            println("  - è‡ªåŠ¨CPUå›é€€")
            println("  - æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–")
            
        else
            # é»˜è®¤GPUä¼˜åŒ–
            optimizer = create_gpu_bayesian_workflow(config_path, "single_objective")
        end
    else
        # é»˜è®¤GPUä¼˜åŒ–
        optimizer = create_gpu_bayesian_workflow(config_path, "single_objective")
    end
    
    println("\nğŸ‰ GPUè´å¶æ–¯ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼")
    println("ğŸ’¡ ç°åœ¨å¯ä»¥ç”¨GPUåŠ é€Ÿçš„æ™ºèƒ½ç®—æ³•æ›¿ä»£ç½‘æ ¼æ‰«æï¼Œè·å¾—æ›´å¿«çš„å‚æ•°ä¼˜åŒ–ï¼")
end